# gemma3.py
import re
import csv
import json
import sys

from datetime import datetime
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

from langchain_ollama import OllamaLLM
from langchain.schema import Document
from langchain.prompts import PromptTemplate

STOP_PHRASE = "ì£¼ì—° ë°°ìš°ë“¤ì˜ ë˜ ë‹¤ë¥¸ ì‘í’ˆ ì´¬ì˜ì§€"

# ===== í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ =====
def clean_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)  # ì œì–´ë¬¸ì ì œê±°
    text = re.sub(r'\s+', ' ', text)  # ê³µë°± ì •ë¦¬
    return text.strip()

# ===== ì‘í’ˆëª… ì¶”ì¶œ + ì •ê·œí™” =====
def extract_work_title(query: str) -> str:
    m = re.search(r"'(.+?)'", query)
    return m.group(1).strip() if m else "default"

# ===== ê³µí†µ: í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸° =====
def fetch_html(url: str, timeout=45000) -> str:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url, timeout=timeout)
        page.wait_for_load_state("networkidle")
        page.wait_for_timeout(1500)
        html = page.content()
        browser.close()
    return html

# ===== ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ =====
def extract_naver_blog_text(url, timeout=20000):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url, timeout=timeout)
        page.wait_for_load_state("networkidle")

        soup = BeautifulSoup(page.content(), "lxml")
        iframe = soup.select_one("iframe#mainFrame")
        if not iframe or not iframe.get("src"):
            browser.close()
            return ""

        iframe_url = f"https://blog.naver.com{iframe['src']}"
        page.goto(iframe_url, timeout=timeout)
        page.wait_for_load_state("networkidle")
        page.wait_for_timeout(2000)
        frame_html = page.content()
        browser.close()

    frame_soup = BeautifulSoup(frame_html, "lxml")
    selectors = [
        "div.se-main-container p",
        "div.se-component span",
        "div.se-module span",
        "div#postViewArea p",
        "div.se_textView p",
    ]
    for sel in selectors:
        elems = frame_soup.select(sel)
        if elems:
            return "\n".join(
                [clean_text(e.get_text(strip=True)) for e in elems if e.get_text(strip=True)]
            )
    return ""

# ===== ì¼ë°˜ í‹°ìŠ¤í† ë¦¬ ë³¸ë¬¸ ì¶”ì¶œ =====
def extract_tistory_text(url, timeout=60000):
    html = fetch_html(url, timeout=timeout)
    soup = BeautifulSoup(html, "lxml")
    selectors = [
        "div.entry-content p",
        "div.article p",
        "div.tt_article_useless_p_margin p",
    ]
    for sel in selectors:
        elems = soup.select(sel)
        if elems:
            text = "\n".join(
                [clean_text(e.get_text(strip=True)) for e in elems if e.get_text(strip=True)]
            )
            if STOP_PHRASE in text:
                text = text.split(STOP_PHRASE)[0]
            return text
    return ""

# ===== ys-dl ì „ìš© íŒŒì„œ =====
def is_probable_place(name: str) -> bool:
    if not name:
        return False
    if len(name.strip()) < 2:
        return False
    bad_keywords = ["ì´¬ì˜ì§€", "ëª©ì°¨", "ì¶œì²˜", "í‹°ìŠ¤í† ë¦¬", "ëŒ“ê¸€", "ì§€ë„"]
    if any(k in name for k in bad_keywords):
        return False
    return True

def extract_ysdl_places(url: str, timeout=45000):
    html = fetch_html(url, timeout=timeout)
    soup = BeautifulSoup(html, "lxml")

    content = soup.select_one("div.entry-content") or soup.select_one("div.article")
    if not content:
        print("[DEBUG] ys-dl ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ëª» ì°¾ìŒ")
        return []

    nodes = list(content.find_all(["h2", "h3", "h4", "figcaption"], recursive=True))

    # ğŸ”¹ "ëª©ì°¨"ë¼ëŠ” ë‹¨ì–´ê°€ ë‚˜ì˜¤ëŠ” ì§€ì  ì´í›„ë§Œ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
    start_idx = 0
    for i, n in enumerate(nodes):
        if "ëª©ì°¨" in clean_text(n.get_text(" ", strip=True)):
            start_idx = i + 1
            break
    nodes = nodes[start_idx:]   # ëª©ì°¨ ì´í›„ë§Œ ì‚¬ìš©

    # STOP_PHRASE ê¸°ì¤€ìœ¼ë¡œ ì ˆë‹¨
    cut_idx = None
    for i, n in enumerate(nodes):
        if n.name in ("h2", "h3", "h4") and STOP_PHRASE in clean_text(n.get_text(" ", strip=True)):
            cut_idx = i
            break
    if cut_idx is not None:
        nodes = nodes[:cut_idx]

    places = []
    for n in nodes:
        if n.name in ("h2", "h3", "h4"):
            title = clean_text(n.get_text(" ", strip=True))
            if is_probable_place(title):
                places.append({"name": title, "address": None})
        elif n.name == "figcaption":
            addr = clean_text(n.get_text(" ", strip=True))
            if places and addr and not places[-1]["address"]:
                places[-1]["address"] = addr

    out = [clean_text(p["name"]) for p in places if is_probable_place(p["name"])]
    seen, dedup = set(), []
    for x in out:
        if x not in seen:
            seen.add(x)
            dedup.append(x)

    return dedup


# ===== search_log.csv ë¡œë”© =====
def load_docs_from_search_log(csv_path, work_title):
    docs, ysdl_rows = [], []
    with open(csv_path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        count = 0   # ğŸ”¹ ì¶”ê°€
        for row in reader:
            if row.get("work_title") and row["work_title"].strip() != work_title.strip():
                continue

            url = row["url"]
            domain = urlparse(url).netloc.lower()

            try:
                if domain.endswith("ys-dl.tistory.com"):
                    places = extract_ysdl_places(url)
                    for place in places:
                        ysdl_rows.append({
                            "TITLE_NM": work_title,
                            "PLACE_NM": clean_text(place),
                            "SRC_URL": [url],
                        })
                elif "blog.naver.com" in domain:
                    text = extract_naver_blog_text(url)
                    if text.strip():
                        docs.append(Document(page_content=text, metadata={"url": url, "title": row["title"], "work": work_title}))
                elif "tistory.com" in domain:
                    text = extract_tistory_text(url)
                    if text.strip():
                        docs.append(Document(page_content=text, metadata={"url": url, "title": row["title"], "work": work_title}))
            except Exception as e:
                print(f"[WARN] Failed to fetch {url}: {e}")

            count += 1
            if count >= 10:   # ğŸ”¹ ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ
                break

    print(f"[INFO] Loaded {len(docs)} docs for {work_title} (ysdl_rows: {len(ysdl_rows)})")
    return docs, ysdl_rows


# ===== ì¥ì†Œëª…ë§Œ LLMìœ¼ë¡œ ì¶”ì¶œ =====
def analyze_and_extract_places(doc: Document, work_title):
    llm = OllamaLLM(model="gemma3")
    template = """
    ì•„ë˜ í…ìŠ¤íŠ¸ì—ì„œ ë“œë¼ë§ˆ/ì˜í™” ì´¬ì˜ì§€ **ì¥ì†Œëª…ë§Œ** JSON ë°°ì—´ë¡œ ì¶”ì¶œí•˜ë¼.
    ê° í•­ëª©ì€ ë°˜ë“œì‹œ PLACE_NM ë§Œ í¬í•¨í•œë‹¤.
    ì¶œë ¥ì€ JSON ë°°ì—´ë§Œ í•˜ë¼.
    í…ìŠ¤íŠ¸:
    {context}
    """
    prompt = PromptTemplate(template=template, input_variables=["context"])
    chain = prompt | llm

    context = doc.page_content[:10000]
    result_json = chain.invoke({"context": context})

    rows = []
    try:
        m = re.search(r"\[.*\]", result_json, re.S)
        if not m:
            return rows
        parsed = json.loads(m.group(0))
        for item in parsed:
            if isinstance(item, dict):
                place = clean_text(item.get("PLACE_NM", "").strip())
            elif isinstance(item, str):
                place = clean_text(item.strip())
            else:
                continue
            if place:
                rows.append({
                    "TITLE_NM": work_title,
                    "PLACE_NM": place,
                    "SRC_URL": [doc.metadata["url"]],
                })
    except Exception as e:
        print(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    return rows

# ===== JSON ì €ì¥ =====
def save_results_as_json(all_rows, output_json="ì´¬ì˜ì§€_ì¶”ì¶œ.json"):
    merged = {}
    for row in all_rows:
        key = (row["TITLE_NM"], clean_text(row["PLACE_NM"]))
        if key not in merged:
            merged[key] = {
                "TITLE_NM": row["TITLE_NM"],
                "PLACE_NM": clean_text(row["PLACE_NM"]),
                "SRC_URL": list(row.get("SRC_URL", [])),
            }
        else:
            if isinstance(row.get("SRC_URL"), list):
                merged[key]["SRC_URL"] = list(set(merged[key]["SRC_URL"] + row["SRC_URL"]))

    filtered = []
    for row in merged.values():
        if any("ys-dl.tistory.com" in src for src in row["SRC_URL"]):
            filtered.append(row)
        elif len(row["SRC_URL"]) >= 2:
            filtered.append(row)

    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(filtered, f, ensure_ascii=False, indent=2)
    print(f"[INFO] Saved {len(filtered)} places to {output_json}")

# ===== ì‹¤í–‰ ë©”ì¸ =====
def main():
    if len(sys.argv) < 2:
        print("Usage: python gemma3.py <ì‘í’ˆëª…>")
        sys.exit(1)

    # ğŸ”¹ ëª¨ë“  argv[1:]ì„ í•©ì³ì„œ ì‘í’ˆëª…ìœ¼ë¡œ ì²˜ë¦¬
    work_title = " ".join(sys.argv[1:]).strip()
    print(f"2ë‹¨ê³„: ì´¬ì˜ì§€ ì¶”ì¶œ ì‹œì‘: {work_title}")

    docs, ysdl_rows = load_docs_from_search_log("search_log.csv", work_title)

    all_rows = []
    all_rows.extend(ysdl_rows)

    for doc in docs:
        rows = analyze_and_extract_places(doc, work_title)
        all_rows.extend(rows)

    save_results_as_json(all_rows, "ì´¬ì˜ì§€_ì¶”ì¶œ.json")


if __name__ == "__main__":
    main()
